---
title: "College_Basketball_Data_Analysis"
authors: 
  - Blessing Amoah
  - Alex Barnard
  - Fengyi Li
format: 
  html:
    code-tools: true
    toc: true
    embed-resources: true
editor: source
---

```{r}
#| include: false

library(tidyverse)
library(dagitty)
library(ggdag)
library(ggplot2)
library(bayesplot)
library(rethinking)
library(ggformula)
```

```{r}
#| include: false
# load data_set
data <- read_csv("summary25_pt.csv")
```

```{r}
# glimpse of the data
glimpse(data)
```
## Data Source


This data includes metrics such as:

- Tempo (pace of play)

- Offensive/Defensive Efficiency (OE/DE)

- Adjusted metrics (AdjOE, AdjDE)

- Efficiency margins

- Team seeds which indicates their tournament ranking.

## Research question

Our goal is to answer the question:

What is the probability that a team ends up in the Final Four based on their adjusted offensive and defensive efficiency?

This will help us understand which team characteristics are most influential in achieving Final Four success, which is critical for predicting tournament outcomes.


## Variables of Interest

- Response variable: Final_four

This is a binary variable indicating whether a team made it to the Final Four (TRUE) or not (FALSE)

- Key predictor: AdjOE thus Adjusted Offensive Efficiency

- Other predictors may include AdjDE, AdjTempo, AdjEM(Efficiency Margin).

```{r}
# Convert Final Four appearance to binary (TRUE if team reached Final Four, FALSE otherwise)

data <- data %>%
  mutate(final_four = ifelse(final_four >= 0, TRUE, FALSE))

# check the class balance
table(data$final_four)
```


```{r}
# causal diagram

offensive_defensive_metrics <- dagitty("dag{
  AdjOE -> Final_Four;
  AdjDE -> Final_Four;
  AdjOE -> AdjEM;
  AdjDE -> AdjEM;
  AdjTempo -> Final_Four;
  AdjEM -> Final_Four;
  Tempo -> AdjTempo;
  OE -> AdjOE;
  DE -> AdjDE;
  AdjOE -> seed;
  AdjDE -> seed;
  seed -> Final_Four
}")
# plot it
ggdag(offensive_defensive_metrics) +
  theme(
    plot.margin = margin(15, 15, 15, 15)
  )
```

Model Description

$$\text{final_four}_i ∼ Binomial(1, p_i)$$

$$logit(p_i) = \beta_1 * AdjOE_i + \beta_2 * AdjDE_i + \beta_3 * AdjEM_i + \beta_4 * AdjTempo_i + \beta_{seed[i]}$$


$$logit(p_i) = \beta_1 * AdjOE_i + \beta_2 * AdjDE_i + \beta_3 * AdjEM_i + \beta_4 * AdjTempo_i + \beta_{seed[i]}$$


Priors

```{r}
clean_data <- data %>%
  drop_na(final_four, seed, AdjOE, AdjDE, AdjEM, AdjTempo)
nrow(clean_data)
```

```{r}
dat <- list(
  final_four = as.integer(clean_data$final_four),
  AdjOE = scale(clean_data$AdjOE),
  AdjDE = scale(clean_data$AdjDE),
  AdjEM = scale(clean_data$AdjEM),
  AdjTempo = scale(clean_data$AdjTempo),
  seed = clean_data$seed,
  N = nrow(clean_data),
  N_seeds = max(clean_data$seed, na.rm = TRUE)
)
```


```{r}
#| include: false
m_mcmc <- ulam(
  alist(
    final_four ~ dbinom(1, p),
    logit(p) <- b1*AdjOE + b2*AdjDE + b3*AdjEM + b4*AdjTempo + b_seed[seed],

    # Priors for main effects
    b1 ~ dnorm(0, 1.5),
    b2 ~ dnorm(0, 1.5),
    b3 ~ dnorm(0, 1.5),
    b4 ~ dnorm(0, 1.5),

    # Hierarchical prior for seed effects
    b_seed[seed] ~ dnorm(0, sigma_seed),
    sigma_seed ~ dexp(1)
  ),
  data = dat,
  chains = 4,
  cores = 4,
  iter = 2000
)
```


```{r}
# View model results
precis(m_mcmc, depth = 2)
# Check trace plots for convergence
traceplot(m_mcmc)

```

## Rtionale for Priors

- Priors for Coefficients (β₁ to β₄)

A Normal(0, 1.5) prior is assigned to each regression coefficient (β₁ to β₄), centering the effects around zero but allowing for moderately large deviations. This helps prevent overfitting while allowing the data to inform the posterior. Although it permits somewhat big impacts, this weakly informative prior focuses on no effect (0). We anticipate that coefficients will fall within a tolerable range because adjusted efficiency measurements are standardized or on comparable scales, and a standard deviation of 1.5 guarantees shrinkage without being overly constrictive.

- Prior for Group-Level Intercept (β_seed[i])

We assume that each seed group can vary significantly because β_seed[i] is taken from a normal distribution with its unique group-level standard deviation.  A hierarchical model with partial pooling is made possible by this structure, which acknowledges that various seeds may have distinct effects on the result while still drawing heavily from the population-level distribution.


Prior Predictive Distribution


```{r}
n_sim <- 10
sigma_seed = dexp(1)

prior_pred_dists <- tibble(
  b1 = rnorm(n_sim, mean = 0, sd = 1.5),
  b2 = rnorm(n_sim, mean = 0, sd = 1.5),
  b3 = rnorm(n_sim, mean = 0, sd = 1.5),
  b4 = rnorm(n_sim, mean = 0, sd = 1.5),
  b_seed = rnorm(n_sim, mean = 0, sigma_seed),
  sim_number = c(1:n_sim)
  )

glimpse(prior_pred_dists)
```

```{r}
# Set number of simulations
n_sim <- 1000

# Simulate prior values for coefficients
b1 <- rnorm(n_sim, 0, 1.5)
b2 <- rnorm(n_sim, 0, 1.5)
b3 <- rnorm(n_sim, 0, 1.5)
b4 <- rnorm(n_sim, 0, 1.5)

# Simulate group-level effects
sigma_seed <- rexp(n_sim, 1)
b_seed <- rnorm(n_sim, 0, sigma_seed)

# Simulate standardized predictor values
AdjOE <- rnorm(n_sim)
AdjDE <- rnorm(n_sim)
AdjEM <- rnorm(n_sim)
AdjTempo <- rnorm(n_sim)

# Linear predictor
logit_p <- b1 * AdjOE + b2 * AdjDE + b3 * AdjEM + b4 * AdjTempo + b_seed

# Transform to probability
p <- 1 / (1 + exp(-logit_p))

# Simulate binary final_four outcome
final_four_sim <- rbinom(n_sim, size = 1, prob = p)

# Make dataframe for plotting
prior_sim_df <- tibble(
  probability = p,
  outcome = final_four_sim
)

# Plot histogram of prior predictive probabilities
ggplot(prior_sim_df, aes(x = probability)) +
  geom_histogram(bins = 40, fill = "maroon", color = "gold") +
  labs(title = "Prior Predictive Distribution for Final Four",
       x = "Simulated Probability (Prior Only)",
       y = "Count") +
  theme_minimal()
```

```{r}
# Teams to simulate per prior draw
n_teams <- 100  

# prior_pred_dists for each team
prior_pred_df <- prior_pred_dists %>%
  slice(rep(1:n(), each = n_teams)) %>%
  mutate(
    team_id = rep(1:n_teams, times = nrow(prior_pred_dists)),
    AdjOE = rnorm(n(), mean = 0, sd = 1),
    AdjDE = rnorm(n(), mean = 0, sd = 1),
    AdjEM = rnorm(n(), mean = 0, sd = 1),
    AdjTempo = rnorm(n(), mean = 0, sd = 1)
  ) %>%
  mutate(
    logit_p = b1 * AdjOE + b2 * AdjDE + b3 * AdjEM + b4 * AdjTempo + b_seed,
    p = 1 / (1 + exp(-logit_p)),
    final_four_sim = rbinom(n(), size = 1, prob = p)
  )
```


```{r}
# Prior predictive probability
gf_point(p ~ team_id | sim_number, 
         data = prior_pred_df,
         alpha = 0.5) +
  labs(
    title = "Prior Predictive Simulations (Final Four Probability)",
    x = "Team ID",
    y = "Simulated Probability (Prior Only)"
  )
```


### Interpretation of the Prior Predictive Distribution

Without utilizing any observed data, the prior predictive distribution shows the likelihood that a team would advance to the Final Four based only on samples from the previous distributions of the model parameters.

#### Key Takeaways:

- The histogram demonstrates that, under the prior alone, the odds of making it to the Final Four fluctuate throughout the whole [0, 1] range, with a notable bulk near **0 and 1**.

- The faceted scatterplot (one panel per simulation) illustrates how the probability of the Final Four is impacted by preconceived notions about team metrics (such as Adjusted Offensive and Defensive Efficiency) for several hypothetical teams.

- The distribution's shape indicates that the priors used (`Exponential(1)` for seed-level variation and `Normal(0,1.5)` for coefficients) are **weakly informative** but **flexible enough** to permit both high and low probability based on the simulated covariates.

- The probability spread demonstrates that the model is not overconfident before to data viewing and that, when updated via posterior estimation, the model can learn from the data.

Because it confirms that our priors encode appropriate uncertainty and do not compel the model to make irrational assumptions, this phase is crucial to Bayesian modeling.  Overly narrow priors may have been indicated if the majority of probability had clustered firmly around 0.5.  They would indicate overconfident, unrealistic priors if they were primarily at 0 or 1.

#### Conclusion:
Our prior predictive check confirms that the selected priors are suitable for learning from the real data in the following stage of the study since they achieve a decent balance between **regularization** and **flexibility**.


In the next steps of the analysis, we will:

- Fit and compare a second model: Exclude one predictor and use WAIC or compare() to evaluate.
- Interpret posterior: Discuss what the estimates mean 
- Fit and compare a second model: Exclude one predictor and use WAIC or compare() to evaluate.
- Write a brief discussion/conclusion: State whether offensive/defensive efficiency truly predicts Final Four outcomes.
-  Add convergence diagnostics explanation.
- Fix Causal diagram feedback given.
- Polish and review all feedback.



